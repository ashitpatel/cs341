{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.optimizers import SGD,Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12977240672257284296\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15771998618\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 12971880801161676139\n",
      "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(path1,path2):\n",
    "    temp_docs = []\n",
    "    labels = []\n",
    "    with open(path1) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for x in data:\n",
    "            temp_docs.append(x['body'])\n",
    "        len1 = len(temp_docs)\n",
    "        print('number of human rights docs is: '+str(len1))\n",
    "        labels = [1]*len1\n",
    "        \n",
    "    with open(path2) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        for x in data:\n",
    "            temp_docs.append(x['body'])\n",
    "        len2 = len(temp_docs)\n",
    "        print('number of non human rights docs is: '+str(len2-len1))\n",
    "        labels = labels + [0]*(len2-len1)\n",
    "    return temp_docs,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of human rights docs is: 65630\n",
      "number of non human rights docs is: 63183\n"
     ]
    }
   ],
   "source": [
    "docs,labels = load_data('/home/tigermlt/CS341/github_repo/CS341/parsed_data.json','/home/tigermlt/CS341/data/data_non_human_rights2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128813\n",
      "128813\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random shuffle the data\n",
    "c = list(zip(docs, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.shuffle(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs, labels = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = list(docs)\n",
    "labels = list(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pad documents to a max length, compute by calculating the maximum document length\n",
    "max_length = 20512\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 300))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger=CSVLogger('bnonb_v1.csv',append=True,separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 20512, 300)        107652300 \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 6153600)           0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 6153601   \n",
      "=================================================================\n",
      "Total params: 113,805,901\n",
      "Trainable params: 6,153,601\n",
      "Non-trainable params: 107,652,300\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['acc'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 109491 samples, validate on 19322 samples\n",
      "Epoch 1/10\n",
      "109491/109491 [==============================] - 99s 906us/step - loss: 0.0026 - acc: 0.9995 - val_loss: 0.1196 - val_acc: 0.9780\n",
      "Epoch 2/10\n",
      "109491/109491 [==============================] - 99s 904us/step - loss: 0.0046 - acc: 0.9992 - val_loss: 0.1525 - val_acc: 0.9746\n",
      "Epoch 3/10\n",
      "109491/109491 [==============================] - 99s 904us/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.1533 - val_acc: 0.9775\n",
      "Epoch 4/10\n",
      "109491/109491 [==============================] - 99s 905us/step - loss: 0.0047 - acc: 0.9991 - val_loss: 0.1586 - val_acc: 0.9757\n",
      "Epoch 5/10\n",
      "109491/109491 [==============================] - 99s 904us/step - loss: 0.0030 - acc: 0.9994 - val_loss: 0.1485 - val_acc: 0.9769\n",
      "Epoch 6/10\n",
      "109491/109491 [==============================] - 99s 906us/step - loss: 0.0033 - acc: 0.9993 - val_loss: 0.1606 - val_acc: 0.9764\n",
      "Epoch 7/10\n",
      "109491/109491 [==============================] - 99s 907us/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.1514 - val_acc: 0.9774\n",
      "Epoch 8/10\n",
      "109491/109491 [==============================] - 99s 907us/step - loss: 0.0042 - acc: 0.9990 - val_loss: 0.1619 - val_acc: 0.9776\n",
      "Epoch 9/10\n",
      "109491/109491 [==============================] - 99s 909us/step - loss: 0.0044 - acc: 0.9990 - val_loss: 0.1759 - val_acc: 0.9762\n",
      "Epoch 10/10\n",
      "109491/109491 [==============================] - 100s 909us/step - loss: 0.0031 - acc: 0.9994 - val_loss: 0.1628 - val_acc: 0.9780\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    # fit the model\n",
    "    model.fit(padded_docs, labels, epochs=10, validation_split = 0.15,batch_size = 128,callbacks=[csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_test = []\n",
    "labels_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of human rights docs is: 7940\n"
     ]
    }
   ],
   "source": [
    "with open('/home/tigermlt/CS341/github_repo/CS341/data/10000.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for x in data:\n",
    "        if x['content'] is not None:\n",
    "            docs_test.append(x['content'])\n",
    "    len1 = len(docs_test)\n",
    "    print('number of human rights docs is: '+str(len1))\n",
    "    labels_test = [1]*len1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7940\n",
      "7940\n"
     ]
    }
   ],
   "source": [
    "print(len(docs_test))\n",
    "print(len(labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare tokenizer\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(docs)\n",
    "# integer encode the documents\n",
    "encoded_docs_test = t.texts_to_sequences(docs_test)\n",
    "# pad documents to a max length, compute by calculating the maximum document length\n",
    "max_length = 20512\n",
    "padded_docs_test = pad_sequences(encoded_docs_test, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7940/7940 [==============================] - 5s 691us/step\n",
      "0.231481399817\n",
      "0.939773299748\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs_test, labels_test,batch_size = 128)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('binary_classification2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load saved model weights\n",
    "def load_trained_model(path):\n",
    "    model = build_model()\n",
    "    model.load_weights(path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test = load_trained_model('/home/tigermlt/CS341/wordEmbedding_keras/binary_classification.h5')\n",
    "# compile the model\n",
    "model_test.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "# evaluate the model\n",
    "loss, accuracy = model_test.evaluate(padded_docs_test, labels_test)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
